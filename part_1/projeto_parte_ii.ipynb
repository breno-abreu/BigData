{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e38d59a9-73ce-4660-b6ca-b22b758dbb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/09 16:34:54 WARN Utils: Your hostname, hadoop-lubuntu resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/06/09 16:34:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/09 16:34:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "\n",
    "\n",
    "# Cria uma sess√£o Spark habilitando Hive support para armazenar dados no Spoark Warehouse\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"projeto_parte_ii\") \\\n",
    "    .config('spark.master', 'local') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5773ff6c-e806-4b05-b9b1-0f0d25dca29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/09 16:35:39 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/06/09 16:35:39 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/06/09 16:35:50 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "24/06/09 16:35:50 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore hadoop@127.0.1.1\n",
      "24/06/09 16:35:51 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark read Hive table\n",
    "spark.sql(\"USE projeto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfdccecb-c70c-474b-8c07-70e1ef4209e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nf_numero: string (nullable = true)\n",
      " |-- nf_data_emissao: date (nullable = true)\n",
      " |-- nf_valor_total: decimal(15,2) (nullable = true)\n",
      " |-- emit_cnpj: string (nullable = true)\n",
      " |-- emit_cep: string (nullable = true)\n",
      " |-- emit_municipio: string (nullable = true)\n",
      " |-- dest_cnpj: string (nullable = true)\n",
      " |-- dest_cep: string (nullable = true)\n",
      " |-- dest_municipio: string (nullable = true)\n",
      " |-- prod_nr_item: integer (nullable = true)\n",
      " |-- prod_cod: string (nullable = true)\n",
      " |-- prod_desc: string (nullable = true)\n",
      " |-- prod_ncm: string (nullable = true)\n",
      " |-- ano: integer (nullable = true)\n",
      " |-- nome_regiao: string (nullable = true)\n",
      " |-- sigla_uf: string (nullable = true)\n",
      " |-- nome_municipio: string (nullable = true)\n",
      " |-- nome_mesoregiao: string (nullable = true)\n",
      " |-- nome_microregiao: string (nullable = true)\n",
      " |-- tipologia_rural_urbana: string (nullable = true)\n",
      " |-- hierarquia_urbana: string (nullable = true)\n",
      " |-- pop: integer (nullable = true)\n",
      " |-- pib: decimal(15,2) (nullable = true)\n",
      " |-- pop_meso: long (nullable = true)\n",
      " |-- pib_meso: decimal(25,2) (nullable = true)\n",
      " |-- prod_unid_nn: string (nullable = true)\n",
      " |-- prod_quant_nn: decimal(19,6) (nullable = true)\n",
      " |-- prod_valor_unit_nn: decimal(19,6) (nullable = true)\n",
      " |-- prod_valor_total_nn: decimal(38,11) (nullable = true)\n",
      " |-- log_prod_quant: double (nullable = true)\n",
      " |-- log_prod_valor_unit: double (nullable = true)\n",
      " |-- scaled_log_prod_quant: double (nullable = true)\n",
      " |-- scaled_log_prod_valor_unit: double (nullable = true)\n",
      " |-- scaled_pop_meso: double (nullable = true)\n",
      " |-- scaled_pib_meso: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM notas_fiscais LIMIT 100000\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d82029b-f397-44de-b89e-20608058d85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- prod_ncm: string (nullable = true)\n",
      " |-- prod_unid: string (nullable = true)\n",
      " |-- prod_quant: double (nullable = true)\n",
      " |-- prod_valor_unit: double (nullable = true)\n",
      " |-- reg_mesoregiao: string (nullable = true)\n",
      " |-- reg_tipologia: string (nullable = true)\n",
      " |-- reg_hierarquia: string (nullable = true)\n",
      " |-- reg_pop: double (nullable = true)\n",
      " |-- reg_pib: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_to_keep = '''\n",
    "prod_ncm, \n",
    "prod_unid_nn as prod_unid, \n",
    "scaled_log_prod_quant as prod_quant, \n",
    "scaled_log_prod_valor_unit as prod_valor_unit,\n",
    "nome_mesoregiao as reg_mesoregiao, \n",
    "tipologia_rural_urbana as reg_tipologia, \n",
    "hierarquia_urbana as reg_hierarquia, \n",
    "scaled_pop_meso as reg_pop,\n",
    "scaled_pib_meso as reg_pib\n",
    "'''\n",
    "\n",
    "df = spark.sql(f\"SELECT {columns_to_keep} FROM notas_fiscais\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdf8b6b8-e4b1-40a8-bf16-44da24bf5b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2697ca98-d111-4d00-98fb-5d9c470b85b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# prod_ncm\n",
    "indexer = StringIndexer(inputCol=\"prod_ncm\", outputCol=\"prod_ncm_indexed\")\n",
    "df = indexer.fit(df).transform(df)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"prod_ncm_indexed\", outputCol=\"prod_ncm_onehot\")\n",
    "df = encoder.fit(df).transform(df)\n",
    "\n",
    "#prod_unid\n",
    "indexer = StringIndexer(inputCol=\"prod_unid\", outputCol=\"prod_unid_indexed\")\n",
    "df = indexer.fit(df).transform(df)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"prod_unid_indexed\", outputCol=\"prod_unid_onehot\")\n",
    "df = encoder.fit(df).transform(df)\n",
    "\n",
    "#reg_mesoregiao\n",
    "indexer = StringIndexer(inputCol=\"reg_mesoregiao\", outputCol=\"reg_mesoregiao_indexed\")\n",
    "df = indexer.fit(df).transform(df)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"reg_mesoregiao_indexed\", outputCol=\"reg_mesoregiao_onehot\")\n",
    "df = encoder.fit(df).transform(df)\n",
    "\n",
    "#reg_tipologia\n",
    "indexer = StringIndexer(inputCol=\"reg_tipologia\", outputCol=\"reg_tipologia_indexed\")\n",
    "df = indexer.fit(df).transform(df)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"reg_tipologia_indexed\", outputCol=\"reg_tipologia_onehot\")\n",
    "df = encoder.fit(df).transform(df)\n",
    "\n",
    "#reg_hierarquia\n",
    "indexer = StringIndexer(inputCol=\"reg_hierarquia\", outputCol=\"reg_hierarquia_indexed\")\n",
    "df = indexer.fit(df).transform(df)\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"reg_hierarquia_indexed\", outputCol=\"reg_hierarquia_onehot\")\n",
    "df = encoder.fit(df).transform(df)\n",
    "\n",
    "#df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9578ed7-1e14-47e2-9844-349fb6f6d201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- prod_ncm: string (nullable = true)\n",
      " |-- prod_unid: string (nullable = true)\n",
      " |-- prod_quant: double (nullable = true)\n",
      " |-- prod_valor_unit: double (nullable = true)\n",
      " |-- reg_mesoregiao: string (nullable = true)\n",
      " |-- reg_tipologia: string (nullable = true)\n",
      " |-- reg_hierarquia: string (nullable = true)\n",
      " |-- reg_pop: double (nullable = true)\n",
      " |-- reg_pib: double (nullable = true)\n",
      " |-- prod_ncm_indexed: double (nullable = false)\n",
      " |-- prod_ncm_onehot: vector (nullable = true)\n",
      " |-- prod_unid_indexed: double (nullable = false)\n",
      " |-- prod_unid_onehot: vector (nullable = true)\n",
      " |-- reg_mesoregiao_indexed: double (nullable = false)\n",
      " |-- reg_mesoregiao_onehot: vector (nullable = true)\n",
      " |-- reg_tipologia_indexed: double (nullable = false)\n",
      " |-- reg_tipologia_onehot: vector (nullable = true)\n",
      " |-- reg_hierarquia_indexed: double (nullable = false)\n",
      " |-- reg_hierarquia_onehot: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f89946d-4584-4101-a4c9-3a5e12f7d378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- prod_ncm: vector (nullable = true)\n",
      " |-- prod_unid: vector (nullable = true)\n",
      " |-- prod_quant: double (nullable = true)\n",
      " |-- prod_valor_unit: double (nullable = true)\n",
      " |-- reg_mesoregiao: vector (nullable = true)\n",
      " |-- reg_tipologia: vector (nullable = true)\n",
      " |-- reg_hierarquia: vector (nullable = true)\n",
      " |-- reg_pop: double (nullable = true)\n",
      " |-- reg_pib: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/09 16:36:24 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "columns_to_keep = '''\n",
    "prod_ncm_onehot as prod_ncm,\n",
    "prod_unid_onehot as prod_unid,\n",
    "prod_quant,\n",
    "prod_valor_unit,\n",
    "reg_mesoregiao_onehot as reg_mesoregiao,\n",
    "reg_tipologia_onehot as reg_tipologia,\n",
    "reg_hierarquia_onehot as reg_hierarquia,\n",
    "reg_pop,\n",
    "reg_pib\n",
    "'''\n",
    "\n",
    "df = spark.sql(f\"SELECT {columns_to_keep} FROM df\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bef5ff5-a3eb-4ff9-8956-4c5773ac4aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fc5d899-0af4-4eaf-88f1-9e9add0439eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f67aa955-5bef-4b6c-a402-b983efcc2e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"prod_ncm\", \"prod_unid\", \"prod_quant\", \"reg_mesoregiao\", \"reg_tipologia\", \"reg_hierarquia\", \"reg_pop\", \"reg_pib\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "df_lr = assembler.transform(df)\n",
    "final_df = df_lr.select(\"features\", \"prod_valor_unit\")\n",
    "\n",
    "train_df, test_df = final_df.randomSplit([0.8, 0.2], seed=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0e4c013-05ba-4577-a549-ef55c208408d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/09 16:36:28 WARN DAGScheduler: Broadcasting large task binary with size 1619.6 KiB\n",
      "24/06/09 16:37:14 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:40 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:40 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:41 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:41 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:41 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:42 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:42 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:42 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:42 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:43 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:43 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:43 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:44 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:44 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:44 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:44 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:45 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:45 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:45 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:45 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:46 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:46 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:46 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:46 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:47 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:47 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:47 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:47 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:47 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:48 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:48 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:48 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:48 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:48 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:49 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:49 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:49 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:49 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:49 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:50 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:50 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:50 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:50 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:50 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:51 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:51 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:51 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:51 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:52 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:52 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:52 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:52 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:53 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:53 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:53 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:53 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:53 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:53 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:54 WARN DAGScheduler: Broadcasting large task binary with size 1620.2 KiB\n",
      "24/06/09 16:37:55 WARN DAGScheduler: Broadcasting large task binary with size 1675.2 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "linear_regressor = LinearRegression(featuresCol=\"features\", labelCol=\"prod_valor_unit\", predictionCol=\"predicted_prod_valor_unit_lr\")\n",
    "linear_regressor_model = linear_regressor.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f9c6932-7872-411b-8002-dfa1d7f1b511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/09 16:38:18 WARN DAGScheduler: Broadcasting large task binary with size 1676.2 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data: 0.560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/09 16:38:50 WARN DAGScheduler: Broadcasting large task binary with size 1676.2 KiB\n",
      "[Stage 78:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on test data: 0.687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_lr = linear_regressor_model.transform(test_df)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"prod_valor_unit\", predictionCol=\"predicted_prod_valor_unit_lr\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions_lr)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data: {:.3f}\".format(rmse))\n",
    "\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"prod_valor_unit\", predictionCol=\"predicted_prod_valor_unit_lr\", metricName=\"r2\")\n",
    "r2 = evaluator_r2.evaluate(predictions_lr)\n",
    "print(\"R-squared (R2) on test data: {:.3f}\".format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20782c69-3de9-47e4-8f1b-9f5668f113ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9d92ebf-4ee1-46ed-aa5e-aad98f5080f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"prod_ncm\", \"prod_unid\", \"prod_quant\", \"reg_mesoregiao\", \"reg_tipologia\", \"reg_hierarquia\", \"reg_pop\", \"reg_pib\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "df_dt = assembler.transform(df)\n",
    "final_df = df_dt.select(\"features\", \"prod_valor_unit\")\n",
    "\n",
    "train_df, test_df = final_df.randomSplit([0.8, 0.2], seed=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3ea7029-fc11-4f98-977c-faf1b961f663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/09 16:39:21 WARN DAGScheduler: Broadcasting large task binary with size 1618.3 KiB\n",
      "24/06/09 16:39:46 WARN DAGScheduler: Broadcasting large task binary with size 1618.4 KiB\n",
      "24/06/09 16:40:04 WARN DAGScheduler: Broadcasting large task binary with size 1623.2 KiB\n",
      "24/06/09 16:40:22 WARN DAGScheduler: Broadcasting large task binary with size 1861.8 KiB\n",
      "24/06/09 16:40:38 WARN MemoryStore: Not enough space to cache rdd_158_0 in memory! (computed 170.1 MiB so far)\n",
      "24/06/09 16:40:38 WARN BlockManager: Persisting block rdd_158_0 to disk instead.\n",
      "24/06/09 16:44:38 WARN MemoryStore: Not enough space to cache rdd_158_0 in memory! (computed 255.3 MiB so far)\n",
      "24/06/09 16:46:52 WARN DAGScheduler: Broadcasting large task binary with size 1862.6 KiB\n",
      "24/06/09 16:46:52 WARN MemoryStore: Not enough space to cache rdd_158_0 in memory! (computed 255.3 MiB so far)\n",
      "24/06/09 16:49:15 WARN DAGScheduler: Broadcasting large task binary with size 1863.3 KiB\n",
      "24/06/09 16:49:15 WARN MemoryStore: Not enough space to cache rdd_158_0 in memory! (computed 255.3 MiB so far)\n",
      "24/06/09 16:51:38 WARN DAGScheduler: Broadcasting large task binary with size 1864.6 KiB\n",
      "24/06/09 16:51:39 WARN MemoryStore: Not enough space to cache rdd_158_0 in memory! (computed 255.3 MiB so far)\n",
      "24/06/09 16:54:15 WARN DAGScheduler: Broadcasting large task binary with size 1867.1 KiB\n",
      "24/06/09 16:54:16 WARN MemoryStore: Not enough space to cache rdd_158_0 in memory! (computed 255.3 MiB so far)\n",
      "24/06/09 16:56:50 WARN DAGScheduler: Broadcasting large task binary with size 1872.1 KiB\n",
      "24/06/09 16:56:51 WARN MemoryStore: Not enough space to cache rdd_158_0 in memory! (computed 255.3 MiB so far)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tree_regressor = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"prod_valor_unit\", predictionCol=\"predicted_prod_valor_unit_tr\", maxDepth=6)\n",
    "tree_regressor_model = tree_regressor.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9857934f-9d36-4175-8fc0-6ebde6ea6bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/09 16:59:11 WARN DAGScheduler: Broadcasting large task binary with size 1621.4 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data: 0.756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/09 16:59:34 WARN DAGScheduler: Broadcasting large task binary with size 1621.4 KiB\n",
      "[Stage 96:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) on test data: 0.430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions_tr = tree_regressor_model.transform(test_df)\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"prod_valor_unit\", predictionCol=\"predicted_prod_valor_unit_tr\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions_tr)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data: {:.3f}\".format(rmse))\n",
    "\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"prod_valor_unit\", predictionCol=\"predicted_prod_valor_unit_tr\", metricName=\"r2\")\n",
    "r2 = evaluator_r2.evaluate(predictions_tr)\n",
    "print(\"R-squared (R2) on test data: {:.3f}\".format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b512151f-4abe-496f-a5e0-10d677803e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 'prod_ncm': 0.0\n",
      "Feature 'prod_unid': 0.0\n",
      "Feature 'prod_quant': 2.4376594880005456e-05\n",
      "Feature 'reg_mesoregiao': 0.0\n",
      "Feature 'reg_tipologia': 5.749129765135653e-05\n",
      "Feature 'reg_hierarquia': 0.0032917995968781244\n",
      "Feature 'reg_pop': 0.0\n",
      "Feature 'reg_pib': 0.0\n"
     ]
    }
   ],
   "source": [
    "feature_importance = tree_regressor_model.featureImportances.toArray()\n",
    "\n",
    "# Show feature importance\n",
    "for i, column in enumerate(assembler.getInputCols()):\n",
    "    print(f\"Feature '{column}': {feature_importance[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4c92e3c-a1a4-445d-b8c7-718a8d0f3266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "  prod_valor_unit: 0.837\n",
      "  reg_tipologia: 0.728\n",
      "  reg_hierarquia: 0.406\n",
      "  reg_pop: 0.395\n",
      "  reg_pib: 0.344\n",
      "  prod_unid: 0.260\n",
      "  prod_ncm: 0.235\n",
      "  prod_quant: 0.112\n",
      "  reg_mesoregiao: 0.029\n"
     ]
    }
   ],
   "source": [
    "coefficients = linear_regressor_model.coefficients\n",
    "intercept = linear_regressor_model.intercept\n",
    "\n",
    "feature_importance_lr = sorted(list(zip(df_lr.columns[:-1], map(abs, coefficients))), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "for feature, importance in feature_importance_lr:\n",
    "    print(\"  {}: {:.3f}\".format(feature, importance))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
